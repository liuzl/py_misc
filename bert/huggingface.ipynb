{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9955477714538574}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are not so happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.37451720237731934}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1('æˆ‘å¾ˆä¸å¼€å¿ƒç»™ä½ å±•ç¤ºè¿™ä¸ªåº“çš„ä½¿ç”¨')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.8385279774665833}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1('We are so happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\")\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zliu/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question.', score: 0.6226, start: 34, end: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zliu/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'SQuAD dataset,', score: 0.5053, start: 147, end: 161\n"
     ]
    }
   ],
   "source": [
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a \"free market.\" I think that the idea of a free market is a bit of a stretch. I think that the idea'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'å¦‚æžœæ˜¯ä¸­æ–‡å‘¢,å°±æ˜¯ä¸­å­¦æ–‡å‘¢,å°±æ˜¯ä¸­å­¦æ–‡å°±å®æœ¬è¶³æ–‡å‘¢,å°±æ˜¯ä¸­ï¿½'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"å¦‚æžœæ˜¯ä¸­æ–‡å‘¢\", max_length=50, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize = pipeline(\"summarization\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Extractive Question Answering is the task of extracting an answer from a text given a question . An example of a question answering dataset is the SQuAD dataset . If you would like to fine-tune a model on a the'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = r\"\"\"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\"\"\"\n",
    "summarize(text, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Extractive Question Answering is the task of extracting an answer from a text given a question . An example of a question answering dataset is the SQuAD dataset . If you would like to fine-tune a model on a the'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Donald Trump has insisted he is not conceding the US election, despite seemingly acknowledging for the first time that Democrat Joe Biden won.\n",
    "\n",
    "\"He won because the Election was Rigged,\" the Republican president wrote on Twitter, repeating unsubstantiated claims of election fraud.\n",
    "\n",
    "About an hour later he said he was not conceding the 3 November vote.\n",
    "\n",
    "He has launched a slew of lawsuits in key states, but has not provided any evidence to back his claims of fraud.\n",
    "\n",
    "All the lawsuits have so far been unsuccessful.\n",
    "\n",
    "On Friday, election officials said the vote was the \"most secure in American history\" and there was \"no evidence that any voting system deleted or lost votes, changed votes or was in any way compromised\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Donald Trump has launched a slew of lawsuits in key states, but has not provided any evidence to back his claims of fraud . Election officials said the vote was the \"most secure in American history\" and there was \"no evidence that any voting system deleted or lost votes or was in any way compromised\"'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Donald Trump has launched a slew of lawsuits in key states, but has not provided any evidence to back his claims of fraud . Election officials said the vote was the \"most secure in American history\" and there was \"no evidence that any voting system deleted or lost votes or was in any way compromised\"'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2627a99bbee941ef92093609d30ec9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1c350fa5cb422fa5dea2330f13eede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=862955157.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fb213a67414011aed36ab0f1fa9106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a8fe521af84d9f81a776128a17ecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bot = pipeline(\"conversational\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation_2 = Conversation(\"What's the last book you have read?\")\n",
    "\n",
    "bot([conversation_1, conversation_2])\n",
    "\n",
    "conversation_1.add_user_input(\"Is it an action movie?\")\n",
    "conversation_2.add_user_input(\"What is the genre of this book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Conversation id: 3c855876-b664-4c44-ad94-7c05145e31e2 \n",
       " user >> Going to the movies tonight - any suggestions? \n",
       " bot >> The Big Lebowski \n",
       " user >> Is it an action movie? \n",
       " bot >> It's a comedy. ,\n",
       " Conversation id: 8756f84f-35f9-4886-9ce0-19e83b642172 \n",
       " user >> What's the last book you have read? \n",
       " bot >> The Last Question \n",
       " user >> What is the genre of this book? \n",
       " bot >> I'm not sure, but I think it's fantasy. ]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot([conversation_1, conversation_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_1.add_user_input(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 3c855876-b664-4c44-ad94-7c05145e31e2 \n",
       "user >> Going to the movies tonight - any suggestions? \n",
       "bot >> The Big Lebowski \n",
       "user >> Is it an action movie? \n",
       "bot >> It's a comedy. \n",
       "user >> How are you today? \n",
       "bot >> I'm good. "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot([conversation_1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_1.add_user_input(\"F**k you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 3c855876-b664-4c44-ad94-7c05145e31e2 \n",
       "user >> Going to the movies tonight - any suggestions? \n",
       "bot >> The Big Lebowski \n",
       "user >> Is it an action movie? \n",
       "bot >> It's a comedy. \n",
       "user >> How are you today? \n",
       "bot >> I'm good. \n",
       "user >> F**k you! \n",
       "bot >> I'm sorry. "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot([conversation_1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_1.add_user_input(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 3c855876-b664-4c44-ad94-7c05145e31e2 \n",
       "user >> Going to the movies tonight - any suggestions? \n",
       "bot >> The Big Lebowski \n",
       "user >> Is it an action movie? \n",
       "bot >> It's a comedy. \n",
       "user >> How are you today? \n",
       "bot >> I'm good. \n",
       "user >> F**k you! \n",
       "bot >> I'm sorry. \n",
       "user >> I Love you! \n",
       "bot >> I love you too! \n",
       "user >> ä½ å¥½ \n",
       "bot >> I love you too! "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot([conversation_1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conversation(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.add_user_input(\"Today is a good day\")\n",
    "bot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.add_user_input(\"How are you doing today?\")\n",
    "bot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? \n",
       "user >> I Love You! \n",
       "bot >> I love you too "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.add_user_input(\"I Love You!\")\n",
    "bot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('68ce6e60-2eba-42b7-b019-62681a4a78f5')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say(text):\n",
    "    c.add_user_input(text)\n",
    "    return bot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? \n",
       "user >> I Love You! \n",
       "bot >> I love you too \n",
       "user >> haha \n",
       "bot >> I love you too "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? \n",
       "user >> I Love You! \n",
       "bot >> I love you too \n",
       "user >> haha \n",
       "bot >> I love you too \n",
       "user >> Good for you \n",
       "bot >> I love you too "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"Good for you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? \n",
       "user >> I Love You! \n",
       "bot >> I love you too \n",
       "user >> haha \n",
       "bot >> I love you too \n",
       "user >> Good for you \n",
       "bot >> I love you too \n",
       "user >> I want to say, Thank you! \n",
       "bot >> I love you too "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"I want to say, Thank you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 68ce6e60-2eba-42b7-b019-62681a4a78f5 \n",
       "user >> How are you \n",
       "bot >> I'm good, how are you? \n",
       "user >> Today is a good day \n",
       "bot >> I'm glad to hear that \n",
       "user >> How are you doing today? \n",
       "bot >> I'm doing well, how about you? \n",
       "user >> I Love You! \n",
       "bot >> I love you too \n",
       "user >> haha \n",
       "bot >> I love you too \n",
       "user >> Good for you \n",
       "bot >> I love you too \n",
       "user >> I want to say, Thank you! \n",
       "bot >> I love you too \n",
       "user >> I hate you! \n",
       "bot >> I love you "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"I hate you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conversation(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "User input added while unprocessed input was existing: \"I hate you\" new input ignored: \"bye\". Set `overwrite` to True to overwrite unprocessed user input\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 51af0358-9e1d-4174-81bd-90afe5c61713 \n",
       "user >> I hate you \n",
       "bot >> I hate you too "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 51af0358-9e1d-4174-81bd-90afe5c61713 \n",
       "user >> I hate you \n",
       "bot >> I hate you too \n",
       "user >> bye \n",
       "bot >> I hate you too "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bcf087a5d84294b0493e76d23dcc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691430.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 20, but you input_length is only 19. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Sam Shleifer writes the best docstring examples in the whole world.'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", device=1)\n",
    "summarizer(\"Sam Shleifer writes the best docstring examples in the whole world.\", min_length=5, max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Sam Shleifer writes the best doc'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"Sam Shleifer writes the best docstring examples in the whole world.\", min_length=5, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the answer to life, the universe and everything'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2text_generator = pipeline(\"text2text-generation\", device=1)\n",
    "text2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
