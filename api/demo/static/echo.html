<!DOCTYPE html>
<html>
<body>
<script type="module" src="./webrecorder.js"></script>
<script>
var ws = new WebSocket(((window.location.protocol === "https:") ? "wss://" : "ws://") + window.location.host + "/ws");
ws.binaryType = "arraybuffer";  // Important: this ensures the audio data is received as raw binary data

// 音频上下文设置
const audioCtx = new (window.AudioContext || window.webkitAudioContext)();

// 创建一个用于播放的AudioBufferSourceNode，但不立即连接
let source = null;

// 用于存储接收到的音频数据的队列
let audioQueue = [];


ws.onmessage = function(event) {
    // 将ArrayBuffer转换为适合AudioContext处理的Float32Array
    const inputData = new Int16Array(event.data);
    const float32AudioData = new Float32Array(inputData.length);

    // 转换 PCM 信号到 [-1,1] 区间的float32数据
    for (let i = 0; i < inputData.length; i++) {
        // 除以0x8000（即32768）将Int16Array的PCM数据转换为Float32 [-1,1]
        float32AudioData[i] = inputData[i] / 0x8000;
    }

    // 将音频数据推送到队列以供稍后播放
    audioQueue.push(float32AudioData);
};

function playAudioData() {
    console.log("playAudioData "+audioQueue.length);
    
    if (audioQueue.length > 0) {

        let allSegments = [];
        let totalSamples = 0;

        // 将队列中的所有音频数据添加到数组中
        while (audioQueue.length) {
            let segment = audioQueue.shift();
            allSegments.push(segment);
            totalSamples += segment.length;  // 计算总样本长度
        }

        // 创建一个新的音频缓冲区来容纳所有片段
        const combinedBuffer = audioCtx.createBuffer(1, totalSamples, 16000);

        // 将所有音频数据复制到大缓冲区中
        let offset = 0;
        allSegments.forEach(segment => {
            combinedBuffer.copyToChannel(segment, 0, offset);
            offset += segment.length;
        });

        console.log(allSegments.length+" segments combined into one buffer of "+totalSamples+" samples")

        // 创建一个源来播放合并后的缓冲区
        const source = audioCtx.createBufferSource();
        source.buffer = combinedBuffer;
        source.connect(audioCtx.destination);

        // 设置播放速率（例如，2倍速）
        source.playbackRate.value = 1.05;

        // 开始播放
        source.start();


        source.onended = function() {
            // 当前音频片段播放完毕，继续播放队列中的下一个
            playAudioData();
        };
    } else {
        // 没有音频数据，等待一会儿再试
        setTimeout(playAudioData, 1000);
    }
}

ws.onopen = function(event) {
    console.log("WebSocket is open now.");
};

ws.onclose = function(event) {
    console.log("WebSocket is closed now.");
};



function startRecording() {

    const recorder = new WebRecorder();
    recorder.OnReceivedData = (data) => {
        if (data && data.length > 0) {
            console.log("Audio data received from recorder, "+ data.length+" bytes");
            ws.send(data);  // Send audio data to server directly
        }
    }
    recorder.start();
    console.log("Recording started");
    playAudioData();

    /*
    setTimeout(function() {
        recorder.stop();
        console.log("Recording stopped");
    }, 3000);
    */
}

</script>

<button onclick="startRecording()">Start Recording</button>
</body>
</html>